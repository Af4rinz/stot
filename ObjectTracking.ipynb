{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project deals with the problem of object tracking in a static-background environment. The goal is to find and track objects in a video sequence. The methods used are based on the work of Nummiaro et al. \\cite{nummiaro2002}, where applying colour distributions, make the model more robust to brief and partial occlusion, rotation, and scale changes, especially efficient and useful in the case of single-target tracking.\n",
    "The videos used in this project are from the MOT17 challenge dataset \\cite{mot17}, available [here](https://motchallenge.net/data/MOT17/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "from src.utils import *\n",
    "import os\n",
    "import time\n",
    "# computation and vision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['MOT17-09-DPM-raw']\n",
    "videos = {}\n",
    "for name in names:\n",
    "    videos[name] = load_video('./MOT/'+name+'.mp4', show=False, f=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle-Filter Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method, based on the work *An Adaptive Colour-Based Particle Filter* \\cite{nummiaro2002}, and follows a top-down approach, where it generates object hypotheses and attempts to verify them with the data. Using the colour distributions, this method only attempts to match only objects that have a similar histogram and build on top of particle filtering.\n",
    "\n",
    "## Definition\n",
    "Particle filtering tracks the state of an object described by a vector $X_t$ while an observation vector $Z_t$ keeps track of all observations up to time $t$. These filters are often used for non-Gaussian posterior density $p(X_t|Z_t)$ and observation density $p(z_t|X_t)$. The key idea is approximating the probability distribution by a weighted samples set $S$ where each sample denotes a hypothetical state of the object with a discrete sampling probability distribution $\\pi$.\n",
    "\n",
    "The sample set evolves by propagating each sample according to a model, then each element is weighted in terms of observation and $N$ samples are drawn by choosing a particular sample with probabily $\\pi^{(n)} = p(z_t|X_t = s_t^{(n)}).$ The mean state of an object is estimated at each time $t$ by:\n",
    "$$ E[S] = \\sum_{n=1}^{N}{\\pi^{(n)}s^{(n)}}$$\n",
    "By modelling uncertainty, the model keeps the options open and can consider multiple hypotheses and choose the closest. By keeping the less likely states in memory briefly, the model can deal with short-term occlusion.\n",
    "\n",
    "### Colour Distribution\n",
    "The colour distribution are used to provide robustness to non-rigidity, rotation, scaling, and partial occlusion. By discretizing the distributions into $m$ bins, histograms produced are produced in HSV colour space to allow for less sensitivity to illumination changes (fewer bins in the V channel). The distribution is determined inside an upright elliptic region with half axes $H_x$ and $H_y$. Smaller weights are assigned to pixels farther from the centre of the region using a weighting function.\n",
    "$$k(r) = \\left \\{ \\begin{array}{ccl}1-r^2 & r < 1 \\\\ 0 & otherwise \\end{array}\\right.$$\n",
    "where $r$ is the distance from the centre of the region.\n",
    "The colour distribution $p_y = {p_y^{(u)}}_{u=1..m}$ is defined as:\n",
    "$$p_y^{(u)} = f \\sum_{i=1}^{I}{k(\\frac{||y-x_i||}{a})\\delta[h(x_i) - u]}$$\n",
    "where $f$ is the normalization factor, $I$ is the number of pixels in region, $\\delta$ is the Kronecker delta function, and $a=\\sqrt{H_x^2+H_y^2}$.\n",
    "Considering the colour histograms $p$ and $q$, the similarity measure is defined using Bhattacharyya coefficient:\n",
    "$$\\rho[p,q] = \\sum_{u=1}^{m}{\\sqrt{p^{(u)}q^{(u)}}}$$\n",
    "and hence the Bhattacharyya distance, which updates the a priori distribution given by particle filter, is:\n",
    "$$d[p,q] = \\sqrt{1-\\rho[p,q]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the distribution represents an ellipse given by $state = {x, y, \\dot{x}, \\dot{y}, H_x, H_y, \\dot{a}}$, where $(x,y)$ is the coordinate of the central point, $(H_x, H_y)$ are the half axes, $(\\dot{x}, \\dot{y})$ is the motion, and $\\dot{a}$ is the scale change.\n",
    "The sample set is then propagated using:\n",
    "$$s_t = A s_{t-1} + w_{t-1}$$\n",
    "where $A$ defines the deterministic component of the model and $w_{t-1}$ is a multivariate Gaussian random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm and Implementation\n",
    "The Particle Filter model is first initialised with a set of samples placed inside the initial location of the target and the colour distribution is calculated for the first frame.\n",
    "Each consequent iteration has four steps:\n",
    "1. **Select**: selects $N$ samples from set $S_{t-1}$ with probability $\\pi_{t-1}^{(n)}$.\n",
    "2. **Propagate**: propagates each sample from set $S'_{t-1}$ by the equation given above.\n",
    "3. **Observe**: observes the colour distributions using the method described.\n",
    "4. **Estimate**: estimates the mean state of $S_t$.\n",
    "Generated outputs are saved in the form of jpg images and stitched together using ffmpeg to create the output video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimisations\n",
    "The main parameter —aside from initial set— is the number of particles. For a low number of particles, the model might 'wander' incorrectly, but for a high number of particles, the model might be too slow to track the target.\n",
    "To increase the speed of the algorithm, a third-size video, sliced to three seconds, is used from the MOT17 dataset \\cite{MOT17}. The generated outputs are for the woman in green (right side of the image) for `particle_count` $\\in [10, 50, 80]$ and the man in black (starting on the left side) with `particle_count` $ = [20, 30]$, to display its performance on occlusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from src.ParticleFilter import ParticleFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MOT17-09-DPM-raw'\n",
    "sframe = 50\n",
    "eframe = 200\n",
    "p_count = 20\n",
    "target = 'man_black'\n",
    "# get_points(videos[name][sframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.copy(videos[name][sframe:eframe])\n",
    "PF = ParticleFilter(frames, particle_count=p_count, init_x=77, init_y=93, init_Hx=17, init_Hy=35, out_path='./output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf_df = pd.DataFrame(columns=['Test', 'Time', 'Particle Count'])\n",
    "# or import from file\n",
    "pf_df = pd.read_csv('./output/pf_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "while PF.f_idx < len(PF.frames) - 1:\n",
    "    PF.select()\n",
    "    PF.propagate()\n",
    "    PF.observe()\n",
    "    PF.estimate()\n",
    "pf_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_df.loc[len(pf_df)] = [(name+'-'+target), pf_time, len(PF.particles)]\n",
    "pf_df.to_csv('./output/pf_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_output(output_name=f'{name}_{PF.particle_count}_{target}.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "For the `woman_green` target tests, the optimal particle count is 50, since 10 particles has reduced accuracy, while 80 particles is both slow and more prone to scale changes (the final frames only track a limb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Time</th>\n",
       "      <th>Particle Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOT17-09-DPM-raw-woman_green</td>\n",
       "      <td>2087.318851</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOT17-09-DPM-raw-woman_green</td>\n",
       "      <td>496.561201</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOT17-09-DPM-raw-woman_green</td>\n",
       "      <td>3197.979189</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOT17-09-DPM-raw-man_black</td>\n",
       "      <td>3441.367042</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MOT17-09-DPM-raw-man_black</td>\n",
       "      <td>3487.032923</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Test         Time  Particle Count\n",
       "0  MOT17-09-DPM-raw-woman_green  2087.318851              50\n",
       "1  MOT17-09-DPM-raw-woman_green   496.561201              10\n",
       "2  MOT17-09-DPM-raw-woman_green  3197.979189              80\n",
       "3    MOT17-09-DPM-raw-man_black  3441.367042              30\n",
       "4    MOT17-09-DPM-raw-man_black  3487.032923              20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import src.KalmanFilter\n",
    "import src.utils\n",
    "reload(src.KalmanFilter)\n",
    "reload(src.utils)\n",
    "from src.utils import *\n",
    "from src.KalmanFilter import KalmanFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MOT17-09-DPM-raw'\n",
    "sframe = 0\n",
    "eframe = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.copy(videos[name][sframe:eframe])\n",
    "background = get_background(frames)\n",
    "approx, sorted_contours = clean_frames(frames, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_pipeline(frames, approx, sorted_contours, object_cnt=1, future=1):\n",
    "    KF_list = []\n",
    "    KF_future = KalmanFilter()\n",
    "    measured_x = []\n",
    "    measured_y = []\n",
    "    prev_pred = [{} for i in range(object_cnt)]\n",
    "    pred_colours = get_index_cmap(object_cnt, 'autumn')\n",
    "    prev_colours = get_index_cmap(object_cnt, 'bone')\n",
    "    x_pos, y_pos = 0, 0\n",
    "    for i in range(object_cnt):\n",
    "        kf = KalmanFilter()\n",
    "        KF_list.append(kf)\n",
    "    for f_idx, f in enumerate(frames):\n",
    "        for i in range(object_cnt):\n",
    "            if i < len(sorted_contours):\n",
    "                cx = sorted_contours[f_idx][i][1]\n",
    "                x, y, w, h = cv2.boundingRect(cx)\n",
    "                x_pos = x + w//2\n",
    "                y_pos = y + h//2\n",
    "                # current measurement\n",
    "                zk = np.array([[x_pos], [y_pos]])\n",
    "                xk, yk = zk\n",
    "                # predict (time update)\n",
    "                curr_predict = KF_list[i].predict()\n",
    "                cpx = curr_predict[0]\n",
    "                cpy = curr_predict[1]\n",
    "                # correct (measurement update)\n",
    "                KF_list[i].correct(zk)\n",
    "                # draw\n",
    "                # rectangle with center given\n",
    "                cv2.rectangle(f, (int(xk), int(yk)), (int(xk)+20, int(yk)+40), pred_colours[i], 3)\n",
    "                cv2.putText(f, \"M\" + str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, pred_colours[i], 1)\n",
    "                cv2.line(f, (int(xk), int(yk)), (int(cpx[0]), int(cpy[0])), pred_colours[i], 1)\n",
    "                cv2.putText(f, \"P\" + str(i), (int(cpx[0]), int(cpy[0])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, pred_colours[i], 1)\n",
    "                prev_pred[i] = {'cpx': cpx[0], 'cpy': cpy[0]}\n",
    "                # draw past pred\n",
    "                cv2.rectangle(f, (int(prev_pred[i]['cpx']), int(prev_pred[i]['cpy'])),\n",
    "                             (int(prev_pred[i]['cpx'])+20, int(prev_pred[i]['cpy'])+40), prev_colours[i], 2)\n",
    "\n",
    "                cpx_future = []\n",
    "                cpy_future = []\n",
    "                cpx_future.append(cpx[0])\n",
    "                cpy_future.append(cpy[0])\n",
    "                # prediction\n",
    "                for j in range(future):\n",
    "                    new_cpx = cpx_future[j] \n",
    "                    new_cpy = cpy_future[j]\n",
    "                    measurement_future = np.array([[new_cpx], [new_cpy]])\n",
    "                    KF_future.correct(measurement_future)\n",
    "                    prediction_future = KF_future.predict()\n",
    "                    cpx_future.append(prediction_future[0])\n",
    "                    cpy_future.append(prediction_future[1])\n",
    "                    # draw\n",
    "                    # cv2.rectangle(f, (int(prediction_future[0][0]), int(prediction_future[1][0])), (int(prediction_future[0][0])+20, int(prediction_future[1][0])+40), (255, 0, 0), 2)\n",
    "                    # cv2.line(f, (int(prediction_future[0][0]), int(prediction_future[1][0])), (int(cpx_future[j]), int(cpy_future[j])), (255, 0, 0), 1)\n",
    "    return frames               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_frames = np.copy(frames)\n",
    "object_cnt = 3\n",
    "outs = kalman_pipeline(over_frames, approx, sorted_contours, object_cnt=object_cnt, future=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(outs):\n",
    "    cv2.imshow('out', f)\n",
    "    cv2.imshow('approx', approx[i])\n",
    "    # if key was Q break\n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, f in enumerate(outs):\n",
    "    cv2.imwrite('./output/'+str(i)+'.jpg', f)\n",
    "save_output(output_name=f'kalman_{name}_{object_cnt}.mp4')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
